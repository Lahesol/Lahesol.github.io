---
title: 주차장에 남은 자리 있나요?
author: Lahesol
date: 2022-01-07 14:28:30 +0900
categories: [Making, Parking_Lot_Detector]
tags: [Machine learing]
pin: false
---

------------------------------------

운전의 시작과 끝은 주차장에서 이루어진다.  

좋은 주차 자리를 차지하려는 것 또한 중요한 일 중 하나이지만 고정석이 아니고서야 늘 원하는 자리에 주차를 하기란 어려운 일이다.  

최근 근무를 시작한 이 건물의 주차장은 복층구조로 되어있는데 위 주차장은 면적이 좁아 자리가 잘 나지 않을 뿐더러 주차 공간을 확인하기 위해 진입하였다가 다시 후진하여 출차헤야 하는 경우가 다수 일어나고 있는 것을 알게 되었다.

소유하고 있던 라즈베리파이4와 웹캠을 이용하여 이러한 문제를 해결할 장치를 만들 수 있지 않을까 하여 본 프로젝트를 시작하게 되었다.

-------------------------

마침 유튜브에서 보게 된 [Parking Space Counter using OpenCV Python](https://www.youtube.com/watch?v=caKnQlCMIYI)에서 OPENCV를 이용하여 이미지를 전처리한 후 구역 내 색상에 따라 차량의 유무를 감지하는 프로그램을 작성하였고 이 내용을 기반으로 실시간 주차 구역 감지기를 만들고자 하였다.

딘, 위 코드는 처리하고자 하는 영상이 수직방향에서 찍어 차량끼리 가리는 일이 없고 각 주차칸의 크기가 일정할 때 적용할 수 있다는 점에 들어 실제로 몇 장의 준비된 사진으로 동작시켜본 결과 

1. 앞 차량에 가려 뒷 차가 없더라도 자리가 없는 것으로 인식 됨
2. 멀리 있는 주차칸은 가까운 주차칸과 크기와 모양이 달라 정확도가 떨어짐
3. 검은 차는 노면과 같이 인식되어 차량의 존재를 감지하지 못 함
4. 건물에 의해 그림자가 지는 경우 노면의 명암이 뚜렷하게 구분되면서 차량으로 인식됨

위 네 가지 문제점으로 인해 만들고자 하는 목적에 맞지 않다 판단을 하였디.

-------------------------

단순히 차량의 유무를 색상의 변화로 알아내지 않고 차량 자체를 검출하는 방식을 이용한다면 위 문제점 중 3번과 4번에 대한 해결이 가능할 것으로 예상되어 시도해보았다. TensorFlow Lite를 이욯해 주차장에 Car이 몇 대 있는지 알 수 있다면 각 구역내에 있는 것이 차인지 아닌지 구분한 후 주차장의 자리가 어떻게 되어있는지도 알 수 있을 것이다.

참고한 자료는 Digikey에서 운영하는 [maker.io](https://www.digikey.kr/en/maker)에 올라온 [How to Perform Object Detection with TensorFlow Lite on Raspberry Pi](https://www.digikey.kr/en/maker/projects/how-to-perform-object-detection-with-tensorflow-lite-on-raspberry-pi/b929e1519c7c43d5b2c6f89984883588)를 참고하였다.
위 링크에서는 pre-Trained Model을 사용하였으며 이는 COCO(Common Objects in Context) Dataset에서 훈련된 Mobilenet V1 모델을 사용한다.

위 방법 또한 적용하여 본 결과 

1. 다양한 오브젝트를 대상으로 한 데이터셋으로서 정석적인 구도에서의 검출은 문제 없음
2. 주차장의 자리를 검출하는 목적 상 차량의 존재 유무만을 판단하기에 맞지 않는 데이터셋임
3. 위에서 바라본 구도를 기준으로 검출하기에 차량을 인식하는데 어려움이 있음

위와 같은 이유로 예제를 그대로 사용하는 것에는 무리가 있다고 판단하였다.

--------------------------------------

위와 같은 이유들과

1. 고정된 장소에서 특정 구역을 ROI로 지정하여 검출해야 함
2. 지정된 ROI 내의 분류 클래스는 2가지
3. 시간대의 변화나 환경의 변화 등 여러 변수 고려 필요
등의 이유로 직접 데이터셋을 제작하여 학습을 시키기로 하였다.

이 단계에서 이미지 분류을 위한 학습은 Alexeyab의 [darknet](https://github.com/AlexeyAB/darknet)을 참조하여 진행하였다.

우선 고려한 것은 클래스 분류의 선정이었다.

1. 주차장 내 총 19 개의 자리를 각각 클래스로 분류
2. 자리와 상관없이 차량의 유무만을 클래스로 분류

영상정보 수집을 위한 카메라가 위에서 내려다보는 구도로 설치되어 있으므로 이전 시도들에서 영향을 주었던 것과 마찬가지로 영상처리를 하는데 변수로 작용할 수 있음을 고려하여 첫 번째 클래스 분류를 채택하였다.

2월 11일부터 2월 19일까지 10분 간격으로 인터벌 촬영하여 얻은 사진 535장에 각 19개의 구역별로 차량의 유, 무를 클래스로 정해 총 38개의 클래스가 되었으며 라벨링 툴은 labelImg, 포맷은 YOLO포맷으로 진행하였다.

--------------------------------------

학습할 데이터셋은 준비되었으나 학습을 진행할 컴퓨터나 서버가 없기에 온라인에서 작업할 수 있는 [GOOGLE COLAB](https://colab.research.google.com)을 이용하였다.
준비된 YOLOv4 모델과 데이터셋, 다크넷 프레임워크를 구글드라이브에 저장해놓고 노트북으로 접속할 때마다 불러오는 방식으로 진행하였는데 모델이나 프레임워크의 특성 탓인지 적은 양의 데이터셋임에도 오랜 시간이 걸려 코랩 프로를 결제하기로 하였다.
월 만 원 정도에 구독 방식이라 길게 사용하지 않을 것으로 생각하여 우선 구독 후 사용하지 않을 때 구독 취소하기로 하였다.
그럼에도 오랜 시간이 걸려 생성된 가중치였지만 같은 장소, 같은 구도에서 촬영한 특성인지 매우 높은 mAP를 가지게 되었으나 Jetson Nano에 적용하여 사용하기에 매우 낮은 FPS를 보여 실제 적용하기에 어려울 것이라 판단하여 다른 방법을 찾아보기로 하였다.

---------------------------------------

YOLOv4를 선택했던 이유는 재작년 프로젝트에 사용했던 기억을 바탕으로 다시 해보기 위함이었는데 더 나은 성능을 위해 찾아보니 YOLOv5가 출시되어 있었다.
Framework를 Darknet이 아닌 Pytorch를 사용하며 YOLOv4보다 더 빠른 학습 시간과 더 작은 용량을 가져 본 프로젝트에 적용하기 위해 [YOLOv5](https://github.com/ultralytics/yolov5)를 참조하여 GOOGLE COLAB에서 다시 진행하였다.
데이터셋의 특성을 고려하여 YOLOv5s, m 모델으로 학습을 진행하였으며 약 50 ~ 150 epoch에서 성능 차이를 보며 변화를 주었다.
약 100 epoch에서 mAP@0.5가 0.99에 도달하였지만 mAP@0.5:95는 0.9에 못미쳤으며 약 140 epoch에서 mAP@0.5:95가 0.907에 도달하여 만족스러운 학습 정도에 도달한 것으로 보았다.

테스트 사진.jpg

-------------------------------------

Jetson Nano에서 동작시킬 코드는 detect.py를 수정하는 방식을 택하였다. 


이런 식으로 수정을 하였으며 검출된 결과를 시리얼로 전송해주면 LoRa를 통해 전송하도록 하였다.

-------------------------------------

LoRa 코드 

통신한_거.jpg

-------------------------------------

실행시 자동 시작?

이거 왜 안되는지

-------------------------------------

검출이 아닌 분류 방식 사용?

-------------------------------------

수정해야할 것.
